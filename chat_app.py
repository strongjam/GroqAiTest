import streamlit as st
from groq import Groq
import time
import re
import base64
from PIL import Image
from io import BytesIO
import requests

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Groq Playground", page_icon="ğŸ®", layout="wide")

# API í‚¤ ì„¤ì •
api_key = "your_groq_api_key_here"

# Groq í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = Groq(api_key=api_key)

# APIì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹±
def get_available_models():
    """Groq APIì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    # ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ (í•­ìƒ í‘œì‹œ)
    default_models = {
        "Llama 3.3 70B": "llama-3.3-70b-versatile",
        "Llama 3.1 70B": "llama-3.1-70b-versatile",
        "Llama 3.1 8B": "llama-3.1-8b-instant",
        "Mixtral 8x7B": "mixtral-8x7b-32768",
        "Llama 3.2 90B Vision": "llama-3.2-90b-vision-preview",
        "Llama 3.2 11B Vision": "llama-3.2-11b-vision-preview",
    }

    try:
        url = "https://api.groq.com/openai/v1/models"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            models_data = response.json()
            api_models = {}

            for model in models_data.get("data", []):
                model_id = model.get("id", "")

                # TTS, Whisper, Guard ëª¨ë¸ ì œì™¸ (ì±„íŒ… API ë¯¸ì§€ì›)
                skip_keywords = ["tts", "whisper", "guard", "safeguard"]
                if any(keyword in model_id.lower() for keyword in skip_keywords):
                    continue

                # ì‚¬ìš©ì ì¹œí™”ì ì¸ ì´ë¦„ ìƒì„±
                if "llama-3.3-70b" in model_id:
                    display_name = "Llama 3.3 70B"
                elif "llama-3.1-70b" in model_id:
                    display_name = "Llama 3.1 70B"
                elif "llama-3.1-8b" in model_id:
                    display_name = "Llama 3.1 8B"
                elif "mixtral-8x7b" in model_id:
                    display_name = "Mixtral 8x7B"
                elif "llama-3.2-90b-vision" in model_id:
                    display_name = "Llama 3.2 90B Vision"
                elif "llama-3.2-11b-vision" in model_id:
                    display_name = "Llama 3.2 11B Vision"
                elif "llama-4-maverick" in model_id:
                    display_name = "Llama 4 Maverick 17B"
                elif "llama-4-scout" in model_id:
                    display_name = "Llama 4 Scout 17B"
                elif "kimi-k2" in model_id:
                    display_name = "Kimi K2"
                elif "compound-mini" in model_id:
                    display_name = "Groq Compound Mini"
                elif "compound" in model_id and "mini" not in model_id:
                    display_name = "Groq Compound"
                elif "gpt-oss-120b" in model_id:
                    display_name = "GPT-OSS 120B"
                elif "gpt-oss-20b" in model_id:
                    display_name = "GPT-OSS 20B"
                elif "qwen3-32b" in model_id:
                    display_name = "Qwen 3 32B"
                elif "allam-2-7b" in model_id:
                    display_name = "Allam 2 7B"
                else:
                    # ê¸°ë³¸ ì´ë¦„ ìƒì„±
                    display_name = model_id.replace("/", " - ").replace("-", " ").title()

                api_models[display_name] = model_id

            # ê¸°ë³¸ ëª¨ë¸ê³¼ API ëª¨ë¸ ë³‘í•© (API ëª¨ë¸ì´ ìš°ì„ )
            merged_models = {**default_models, **api_models}
            return merged_models
        else:
            return default_models
    except Exception as e:
        return default_models

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
AVAILABLE_MODELS = get_available_models()

# ëª¨ë¸ë³„ ì•„ì´ì½˜ (ë™ì ìœ¼ë¡œ ìƒì„±)
def get_model_icon(model_name):
    """ëª¨ë¸ ì´ë¦„ì— ë”°ë¼ ì•„ì´ì½˜ ë°˜í™˜"""
    if "Tts" in model_name or "TTS" in model_name:
        return "ğŸ”Š"
    elif "Vision" in model_name:
        return "ğŸ‘ï¸"
    elif "Llama 4" in model_name:
        return "ğŸ¦™âœ¨"
    elif "Llama" in model_name:
        return "ğŸ¦™"
    elif "Mixtral" in model_name:
        return "ğŸŒ€"
    elif "Gemma" in model_name:
        return "ğŸ’"
    elif "Qwen" in model_name:
        return "ğŸ‰"
    elif "Kimi" in model_name:
        return "ğŸŒ™"
    elif "Compound" in model_name:
        return "âš¡"
    elif "GPT-OSS" in model_name:
        return "ğŸ”“"
    elif "Allam" in model_name:
        return "ğŸŒ"
    else:
        return "ğŸ¤–"

# TTS ëª¨ë¸ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
def is_tts_model(model_name):
    """ëª¨ë¸ì´ TTS ëª¨ë¸ì¸ì§€ í™•ì¸"""
    return "tts" in model_name.lower()

# ê¸°ë³¸ ëª¨ë¸ ì•„ì´ì½˜ (ìºì‹±ìš©)
MODEL_ICONS = {model: get_model_icon(model) for model in AVAILABLE_MODELS.keys()}

# ëª¨ë¸ë³„ ì„¤ëª… (ê¸°ë³¸ ì •ë³´)
DEFAULT_MODEL_DESCRIPTIONS = {
    "Llama 3.3 70B": {
        "description": "Metaì˜ ìµœì‹  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸",
        "strengths": "ê³ í’ˆì§ˆ ì‘ë‹µ, ë³µì¡í•œ ì¶”ë¡ , ì°½ì˜ì  ì‘ì—…",
        "best_for": "ì „ë¬¸ì ì¸ ì§ˆë¬¸, ê¸´ ëŒ€í™”, ë³µì¡í•œ ë¬¸ì œ í•´ê²°",
        "speed": "ë³´í†µ",
        "quality": "â­â­â­â­â­"
    },
    "Llama 3.1 70B": {
        "description": "ì•ˆì •ì ì´ê³  ê°•ë ¥í•œ ëŒ€í˜• ëª¨ë¸",
        "strengths": "ê· í˜•ì¡íŒ ì„±ëŠ¥, ì‹ ë¢°ì„± ë†’ì€ ì‘ë‹µ",
        "best_for": "ì¼ë°˜ì ì¸ ì§ˆë¬¸, ë¶„ì„, ìš”ì•½",
        "speed": "ë³´í†µ",
        "quality": "â­â­â­â­â­"
    },
    "Llama 3.1 8B": {
        "description": "ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ì†Œí˜• ëª¨ë¸",
        "strengths": "ë¹ ë¥¸ ì‘ë‹µ ì†ë„, ë‚®ì€ ì§€ì—°ì‹œê°„",
        "best_for": "ê°„ë‹¨í•œ ì§ˆë¬¸, ë¹ ë¥¸ ëŒ€í™”, ì‹¤ì‹œê°„ ì‘ë‹µ",
        "speed": "ë§¤ìš° ë¹ ë¦„ âš¡",
        "quality": "â­â­â­â­"
    },
    "Mixtral 8x7B": {
        "description": "Mistral AIì˜ ê³ ì„±ëŠ¥ MoE ëª¨ë¸",
        "strengths": "ë‹¤ì–‘í•œ ì‘ì—… ì²˜ë¦¬, ë©€í‹°íƒœìŠ¤í‚¹",
        "best_for": "ì½”ë”©, ê¸°ìˆ  ë¬¸ì„œ, ë‹¤êµ­ì–´ ì§€ì›",
        "speed": "ë¹ ë¦„",
        "quality": "â­â­â­â­â­"
    },
    "Llama 3.2 90B Vision": {
        "description": "ë¹„ì „ ê¸°ëŠ¥ì´ ìˆëŠ” ëŒ€í˜• ë©€í‹°ëª¨ë‹¬ ëª¨ë¸",
        "strengths": "ì´ë¯¸ì§€ ì´í•´, ì‹œê°ì  ì¶”ë¡ ",
        "best_for": "ì´ë¯¸ì§€ ë¶„ì„, ì‹œê°ì  ì§ˆë¬¸ ë‹µë³€",
        "speed": "ë³´í†µ",
        "quality": "â­â­â­â­â­"
    },
    "Llama 3.2 11B Vision": {
        "description": "ë¹ ë¥¸ ë¹„ì „ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•œ ëª¨ë¸",
        "strengths": "ë¹ ë¥¸ ì´ë¯¸ì§€ ì²˜ë¦¬, íš¨ìœ¨ì ì¸ ë¹„ì „ ì‘ì—…",
        "best_for": "ë¹ ë¥¸ ì´ë¯¸ì§€ ë¶„ì„, ì‹¤ì‹œê°„ ë¹„ì „ ì‘ì—…",
        "speed": "ë¹ ë¦„",
        "quality": "â­â­â­â­"
    }
}

def get_model_description(model_name):
    """ëª¨ë¸ ì´ë¦„ì— ë”°ë¼ ì„¤ëª… ìƒì„±"""
    # ê¸°ë³¸ ì„¤ëª…ì´ ìˆìœ¼ë©´ ë°˜í™˜
    if model_name in DEFAULT_MODEL_DESCRIPTIONS:
        return DEFAULT_MODEL_DESCRIPTIONS[model_name]

    # ë™ì ìœ¼ë¡œ ì„¤ëª… ìƒì„±
    model_lower = model_name.lower()

    # TTS ëª¨ë¸
    if "tts" in model_lower:
        return {
            "description": "í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” TTS ëª¨ë¸",
            "strengths": "ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ìƒì„±, ë‹¤ì–‘í•œ ëª©ì†Œë¦¬",
            "best_for": "í…ìŠ¤íŠ¸ ìŒì„± ë³€í™˜, ì˜¤ë””ì˜¤ ìƒì„±",
            "speed": "ë¹ ë¦„",
            "quality": "â­â­â­â­"
        }

    # Vision ëª¨ë¸
    elif "vision" in model_lower:
        return {
            "description": "ë©€í‹°ëª¨ë‹¬ ë¹„ì „ ëª¨ë¸",
            "strengths": "ì´ë¯¸ì§€ ì´í•´, ì‹œê°ì  ë¶„ì„",
            "best_for": "ì´ë¯¸ì§€ ë¶„ì„, ì‹œê°ì  ì§ˆë¬¸ ë‹µë³€",
            "speed": "ë³´í†µ",
            "quality": "â­â­â­â­"
        }

    # Llama 4 ëª¨ë¸
    elif "llama 4" in model_lower or "llama-4" in model_lower:
        if "maverick" in model_lower:
            return {
                "description": "Metaì˜ Llama 4 Maverick ëª¨ë¸",
                "strengths": "ìµœì‹  ì•„í‚¤í…ì²˜, í–¥ìƒëœ ì¶”ë¡  ëŠ¥ë ¥",
                "best_for": "ë³µì¡í•œ ë¬¸ì œ í•´ê²°, ì „ë¬¸ì ì¸ ëŒ€í™”",
                "speed": "ë¹ ë¦„",
                "quality": "â­â­â­â­â­"
            }
        elif "scout" in model_lower:
            return {
                "description": "Metaì˜ Llama 4 Scout ëª¨ë¸",
                "strengths": "ë¹ ë¥¸ íƒìƒ‰, íš¨ìœ¨ì ì¸ ì²˜ë¦¬",
                "best_for": "ë¹ ë¥¸ ì§ˆë¬¸ ë‹µë³€, ì¼ë°˜ ëŒ€í™”",
                "speed": "ë§¤ìš° ë¹ ë¦„ âš¡",
                "quality": "â­â­â­â­"
            }
    # Llama ëª¨ë¸
    elif "llama" in model_lower:
        if "70b" in model_lower or "90b" in model_lower:
            return {
                "description": "Metaì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸",
                "strengths": "ê³ í’ˆì§ˆ ì‘ë‹µ, ë³µì¡í•œ ì¶”ë¡ ",
                "best_for": "ì „ë¬¸ì ì¸ ì§ˆë¬¸, ë³µì¡í•œ ì‘ì—…",
                "speed": "ë³´í†µ",
                "quality": "â­â­â­â­â­"
            }
        else:
            return {
                "description": "Metaì˜ íš¨ìœ¨ì ì¸ ì–¸ì–´ ëª¨ë¸",
                "strengths": "ë¹ ë¥¸ ì‘ë‹µ, íš¨ìœ¨ì ì¸ ì²˜ë¦¬",
                "best_for": "ì¼ë°˜ì ì¸ ì§ˆë¬¸, ë¹ ë¥¸ ëŒ€í™”",
                "speed": "ë¹ ë¦„",
                "quality": "â­â­â­â­"
            }

    # Mixtral ëª¨ë¸
    elif "mixtral" in model_lower:
        return {
            "description": "Mistral AIì˜ MoE ëª¨ë¸",
            "strengths": "ë‹¤ì–‘í•œ ì‘ì—…, ì½”ë”© ì§€ì›",
            "best_for": "ì½”ë”©, ê¸°ìˆ  ë¬¸ì„œ, ë³µì¡í•œ ì‘ì—…",
            "speed": "ë¹ ë¦„",
            "quality": "â­â­â­â­â­"
        }

    # Gemma ëª¨ë¸
    elif "gemma" in model_lower:
        return {
            "description": "Googleì˜ ê²½ëŸ‰ ì–¸ì–´ ëª¨ë¸",
            "strengths": "íš¨ìœ¨ì ì¸ ì²˜ë¦¬, ë¹ ë¥¸ ì‘ë‹µ",
            "best_for": "ì¼ë°˜ ëŒ€í™”, ë¹ ë¥¸ ì‘ì—…",
            "speed": "ë§¤ìš° ë¹ ë¦„ âš¡",
            "quality": "â­â­â­â­"
        }

    # Qwen ëª¨ë¸
    elif "qwen" in model_lower:
        return {
            "description": "Alibabaì˜ ë‹¤êµ­ì–´ ì–¸ì–´ ëª¨ë¸",
            "strengths": "ë‹¤êµ­ì–´ ì§€ì›, ë‹¤ì–‘í•œ ì‘ì—…",
            "best_for": "ë‹¤êµ­ì–´ ì²˜ë¦¬, ì¼ë°˜ ì‘ì—…",
            "speed": "ë³´í†µ",
            "quality": "â­â­â­â­"
        }

    # Kimi ëª¨ë¸
    elif "kimi" in model_lower:
        return {
            "description": "Moonshot AIì˜ ì¥ë¬¸ë§¥ ì–¸ì–´ ëª¨ë¸",
            "strengths": "ê¸´ ë¬¸ë§¥ ì´í•´, ë³µì¡í•œ ëŒ€í™”",
            "best_for": "ê¸´ ë¬¸ì„œ ë¶„ì„, ë³µì¡í•œ ì¶”ë¡ ",
            "speed": "ë³´í†µ",
            "quality": "â­â­â­â­â­"
        }

    # Groq Compound ëª¨ë¸
    elif "compound" in model_lower:
        return {
            "description": "Groqì˜ ìµœì í™”ëœ ì–¸ì–´ ëª¨ë¸",
            "strengths": "ì´ˆê³ ì† ì¶”ë¡ , íš¨ìœ¨ì ì¸ ì²˜ë¦¬",
            "best_for": "ë¹ ë¥¸ ì‘ë‹µ, ì‹¤ì‹œê°„ ëŒ€í™”",
            "speed": "ì´ˆê³ ì† âš¡âš¡",
            "quality": "â­â­â­â­â­"
        }

    # GPT-OSS ëª¨ë¸
    elif "gpt-oss" in model_lower:
        return {
            "description": "ì˜¤í”ˆì†ŒìŠ¤ GPT ìŠ¤íƒ€ì¼ ëª¨ë¸",
            "strengths": "ê°•ë ¥í•œ ì–¸ì–´ ì´í•´, ë²”ìš© ì‘ì—…",
            "best_for": "ì¼ë°˜ ëŒ€í™”, ë‹¤ì–‘í•œ ì‘ì—…",
            "speed": "ë³´í†µ",
            "quality": "â­â­â­â­â­"
        }

    # Allam ëª¨ë¸
    elif "allam" in model_lower:
        return {
            "description": "IBMì˜ ë‹¤êµ­ì–´ ì–¸ì–´ ëª¨ë¸",
            "strengths": "ì•„ëì–´ ì§€ì›, ë‹¤êµ­ì–´ ì²˜ë¦¬",
            "best_for": "ë‹¤êµ­ì–´ ì‘ì—…, ë¬¸í™”ì  ì´í•´",
            "speed": "ë¹ ë¦„",
            "quality": "â­â­â­â­"
        }

    # ê¸°íƒ€ ëª¨ë¸
    else:
        return {
            "description": "ì–¸ì–´ ëª¨ë¸",
            "strengths": "ë‹¤ì–‘í•œ ì‘ì—… ì²˜ë¦¬",
            "best_for": "ì¼ë°˜ì ì¸ ì§ˆë¬¸, ëŒ€í™”",
            "speed": "ë³´í†µ",
            "quality": "â­â­â­"
        }

# ëª¨ë¸ ì„¤ëª… ë”•ì…”ë„ˆë¦¬ ìƒì„±
MODEL_DESCRIPTIONS = {model: get_model_description(model) for model in AVAILABLE_MODELS.keys()}

# ì¤‘êµ­ì–´/ì¼ë³¸ì–´ í•œì ê°ì§€ ë° ì œê±° í•¨ìˆ˜
def detect_and_clean_cjk(text):
    """ì¤‘êµ­ì–´/ì¼ë³¸ì–´ í•œìë¥¼ ê°ì§€í•˜ê³  ê²½ê³  í‘œì‹œ"""
    cjk_pattern = re.compile(r'[\u4E00-\u9FFF\u3400-\u4DBF]')
    found_cjk = cjk_pattern.findall(text)

    if found_cjk:
        unique_chars = list(set(found_cjk))
        st.warning(f"âš ï¸ ì‘ë‹µì— ì¤‘êµ­ì–´/ì¼ë³¸ì–´ í•œìê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {', '.join(unique_chars)}")
        cleaned_text = cjk_pattern.sub('?', text)
        return cleaned_text, True

    return text, False

# ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
def encode_image(image):
    """PIL Imageë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜"""
    buffered = BytesIO()
    image.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

if "selected_model" not in st.session_state:
    st.session_state.selected_model = "Llama 3.3 70B"

if "disabled_models" not in st.session_state:
    st.session_state.disabled_models = set()

if "temperature" not in st.session_state:
    st.session_state.temperature = 0.7

if "max_tokens" not in st.session_state:
    st.session_state.max_tokens = 1024

# ì œëª©
st.title("ğŸ® Groq Playground")
st.caption("AI ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° ì‹¤í—˜ í™˜ê²½")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")

    st.subheader("ğŸ¤– ëª¨ë¸ ì„¤ì •")

    # ë‹¨ì¼ ëª¨ë¸ ì„ íƒ
    available_models = [m for m in AVAILABLE_MODELS.keys() if m not in st.session_state.disabled_models]

    # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ì´ ë¹„í™œì„±í™”ë˜ì—ˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ë³€ê²½
    if available_models and st.session_state.selected_model not in available_models:
        st.session_state.selected_model = available_models[0]
        st.warning(f"âš ï¸ ì´ì „ì— ì„ íƒí•œ ëª¨ë¸ì´ ë¹„í™œì„±í™”ë˜ì–´ '{available_models[0]}'ë¡œ ìë™ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")

    if available_models:
        selected_model = st.selectbox(
            "ëª¨ë¸ ì„ íƒ",
            available_models,
            index=available_models.index(st.session_state.selected_model) if st.session_state.selected_model in available_models else 0,
            key="single_model_select"
        )
        st.session_state.selected_model = selected_model

        # ì„ íƒëœ ëª¨ë¸ ì •ë³´ í‘œì‹œ
        if selected_model in MODEL_DESCRIPTIONS:
            model_info = MODEL_DESCRIPTIONS[selected_model]
            with st.expander("â„¹ï¸ ëª¨ë¸ ì •ë³´", expanded=False):
                st.markdown(f"**{model_info['description']}**")
                st.markdown(f"**í’ˆì§ˆ:** {model_info['quality']}")
                st.markdown(f"**ì†ë„:** {model_info['speed']}")
                st.markdown(f"**ê°•ì :** {model_info['strengths']}")
                st.markdown(f"**ì¶”ì²œ ìš©ë„:** {model_info['best_for']}")
    else:
        st.error("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")

    st.markdown("---")

    # ì˜¨ë„ ì„¤ì •
    st.session_state.temperature = st.slider(
        "Temperature",
        min_value=0.0,
        max_value=2.0,
        value=st.session_state.temperature,
        step=0.1,
        help="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ìˆê³ , ë†’ì„ìˆ˜ë¡ ì°½ì˜ì ì¸ ì‘ë‹µ"
    )

    # ìµœëŒ€ í† í°
    st.session_state.max_tokens = st.slider(
        "Max Tokens",
        min_value=256,
        max_value=4096,
        value=st.session_state.max_tokens,
        step=256,
        help="ì‘ë‹µì˜ ìµœëŒ€ ê¸¸ì´"
    )

    st.markdown("---")

    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.markdown("---")
    st.subheader("ğŸ“Š í†µê³„")
    st.metric("ë©”ì‹œì§€ ìˆ˜", len(st.session_state.messages))
    st.metric("í˜„ì¬ ëª¨ë¸", st.session_state.selected_model)
    st.metric("Temperature", f"{st.session_state.temperature:.1f}")

    # ëª¨ë¸ ë¹„êµ ê°€ì´ë“œ (ë™ì  ìƒì„±)
    with st.expander("ğŸ“‹ ëª¨ë¸ ë¹„êµ ê°€ì´ë“œ"):
        st.markdown("### ğŸ“š ì „ì²´ ëª¨ë¸ ëª©ë¡")

        # ëª¨ë“  ëª¨ë¸ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
        vision_models = []
        llama_large_models = []
        llama_small_models = []
        mixtral_models = []
        gemma_models = []
        qwen_models = []
        other_models = []

        for model_name in AVAILABLE_MODELS.keys():
            if "Vision" in model_name:
                vision_models.append(model_name)
            elif "Llama" in model_name:
                if "70b" in model_name.lower() or "90b" in model_name.lower() or "3.3" in model_name:
                    llama_large_models.append(model_name)
                else:
                    llama_small_models.append(model_name)
            elif "Mixtral" in model_name:
                mixtral_models.append(model_name)
            elif "Gemma" in model_name:
                gemma_models.append(model_name)
            elif "Qwen" in model_name:
                qwen_models.append(model_name)
            else:
                other_models.append(model_name)

        # ì¹´í…Œê³ ë¦¬ë³„ ì¶œë ¥
        if llama_large_models:
            st.markdown("#### ğŸ¦™ Llama ëŒ€í˜• ëª¨ë¸ (70B+)")
            for model in llama_large_models:
                desc = MODEL_DESCRIPTIONS.get(model, {})
                icon = get_model_icon(model)
                st.markdown(f"**{icon} {model}**")
                st.markdown(f"- {desc.get('description', '')}")
                st.markdown(f"- í’ˆì§ˆ: {desc.get('quality', '')} | ì†ë„: {desc.get('speed', '')}")
                st.markdown(f"- ì¶”ì²œ: {desc.get('best_for', '')}")
                st.markdown("")

        if llama_small_models:
            st.markdown("#### ğŸ¦™ Llama ì†Œí˜•/ì¤‘í˜• ëª¨ë¸")
            for model in llama_small_models:
                desc = MODEL_DESCRIPTIONS.get(model, {})
                icon = get_model_icon(model)
                st.markdown(f"**{icon} {model}**")
                st.markdown(f"- {desc.get('description', '')}")
                st.markdown(f"- í’ˆì§ˆ: {desc.get('quality', '')} | ì†ë„: {desc.get('speed', '')}")
                st.markdown(f"- ì¶”ì²œ: {desc.get('best_for', '')}")
                st.markdown("")

        if mixtral_models:
            st.markdown("#### ğŸŒ€ Mixtral ëª¨ë¸")
            for model in mixtral_models:
                desc = MODEL_DESCRIPTIONS.get(model, {})
                icon = get_model_icon(model)
                st.markdown(f"**{icon} {model}**")
                st.markdown(f"- {desc.get('description', '')}")
                st.markdown(f"- í’ˆì§ˆ: {desc.get('quality', '')} | ì†ë„: {desc.get('speed', '')}")
                st.markdown(f"- ì¶”ì²œ: {desc.get('best_for', '')}")
                st.markdown("")

        if vision_models:
            st.markdown("#### ğŸ‘ï¸ Vision ëª¨ë¸ (ì´ë¯¸ì§€ ë¶„ì„)")
            for model in vision_models:
                desc = MODEL_DESCRIPTIONS.get(model, {})
                icon = get_model_icon(model)
                st.markdown(f"**{icon} {model}**")
                st.markdown(f"- {desc.get('description', '')}")
                st.markdown(f"- í’ˆì§ˆ: {desc.get('quality', '')} | ì†ë„: {desc.get('speed', '')}")
                st.markdown(f"- ì¶”ì²œ: {desc.get('best_for', '')}")
                st.markdown("")

        if gemma_models:
            st.markdown("#### ğŸ’ Gemma ëª¨ë¸")
            for model in gemma_models:
                desc = MODEL_DESCRIPTIONS.get(model, {})
                icon = get_model_icon(model)
                st.markdown(f"**{icon} {model}**")
                st.markdown(f"- {desc.get('description', '')}")
                st.markdown(f"- í’ˆì§ˆ: {desc.get('quality', '')} | ì†ë„: {desc.get('speed', '')}")
                st.markdown(f"- ì¶”ì²œ: {desc.get('best_for', '')}")
                st.markdown("")

        if qwen_models:
            st.markdown("#### ğŸ‰ Qwen ëª¨ë¸")
            for model in qwen_models:
                desc = MODEL_DESCRIPTIONS.get(model, {})
                icon = get_model_icon(model)
                st.markdown(f"**{icon} {model}**")
                st.markdown(f"- {desc.get('description', '')}")
                st.markdown(f"- í’ˆì§ˆ: {desc.get('quality', '')} | ì†ë„: {desc.get('speed', '')}")
                st.markdown(f"- ì¶”ì²œ: {desc.get('best_for', '')}")
                st.markdown("")

        if other_models:
            st.markdown("#### ğŸ¤– ê¸°íƒ€ ëª¨ë¸")
            for model in other_models:
                desc = MODEL_DESCRIPTIONS.get(model, {})
                icon = get_model_icon(model)
                st.markdown(f"**{icon} {model}**")
                st.markdown(f"- {desc.get('description', '')}")
                st.markdown(f"- í’ˆì§ˆ: {desc.get('quality', '')} | ì†ë„: {desc.get('speed', '')}")
                st.markdown(f"- ì¶”ì²œ: {desc.get('best_for', '')}")
                st.markdown("")

        # í†µê³„ ì •ë³´
        st.markdown("---")
        st.markdown(f"**ì „ì²´ ëª¨ë¸ ìˆ˜:** {len(AVAILABLE_MODELS)}ê°œ")

        # ë¹ ë¥¸ ì„ íƒ ê°€ì´ë“œ
        st.markdown("---")
        st.markdown("### ğŸ¯ ë¹ ë¥¸ ì„ íƒ ê°€ì´ë“œ")

        # ìš©ë„ë³„ ì¶”ì²œ
        fast_models = []
        quality_models = []
        coding_models = []

        for model_name in AVAILABLE_MODELS.keys():
            desc = MODEL_DESCRIPTIONS.get(model_name, {})
            speed = desc.get("speed", "")
            quality = desc.get("quality", "")

            if "ë¹ ë¦„" in speed or "âš¡" in speed:
                fast_models.append(model_name)
            if quality == "â­â­â­â­â­":
                quality_models.append(model_name)
            if "Mixtral" in model_name or ("Llama" in model_name and ("70b" in model_name.lower() or "90b" in model_name.lower())):
                coding_models.append(model_name)

        if fast_models:
            st.markdown("**âš¡ ì†ë„ ì¤‘ìš”:** " + ", ".join(fast_models[:3]))
        if quality_models:
            st.markdown("**â­ í’ˆì§ˆ ì¤‘ìš”:** " + ", ".join(quality_models[:3]))
        if coding_models:
            st.markdown("**ğŸ’» ì½”ë”© ì‘ì—…:** " + ", ".join(coding_models[:3]))
        if vision_models:
            st.markdown("**ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„:** " + ", ".join(vision_models[:2]))

    # ë¹„í™œì„±í™”ëœ ëª¨ë¸ ì •ë³´
    if st.session_state.disabled_models:
        st.markdown("---")
        st.subheader("âš ï¸ ë¹„í™œì„±í™”ëœ ëª¨ë¸")
        for model in st.session_state.disabled_models:
            st.text(f"â€¢ {model}")

        if st.button("ğŸ”“ ë¹„í™œì„±í™” ëª¨ë¸ ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.disabled_models = set()
            st.success("ë¹„í™œì„±í™”ëœ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()

    # ìºì‹œ ë° ì „ì²´ ì´ˆê¸°í™” ë²„íŠ¼
    st.markdown("---")
    if st.button("ğŸ”„ ìºì‹œ ë° ëª¨ë¸ ëª©ë¡ ìƒˆë¡œê³ ì¹¨", use_container_width=True):
        # ìºì‹œ í´ë¦¬ì–´
        st.cache_data.clear()
        # ë¹„í™œì„±í™” ëª©ë¡ ì´ˆê¸°í™”
        st.session_state.disabled_models = set()
        st.success("ìºì‹œê°€ í´ë¦¬ì–´ë˜ê³  ëª¨ë¸ ëª©ë¡ì´ ìƒˆë¡œê³ ì¹¨ë©ë‹ˆë‹¤!")
        st.rerun()

# ë©”ì¸ ì˜ì—­ - ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
st.markdown("---")

# ì´ë¯¸ì§€ ì—…ë¡œë“œ ì˜ì—­ - Vision ëª¨ë¸ì¼ ë•Œë§Œ í‘œì‹œ
uploaded_file = None
if "Vision" in st.session_state.selected_model:
    uploaded_file = st.file_uploader(
        "ğŸ“ ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)",
        type=["png", "jpg", "jpeg", "webp"],
        help="Vision ëª¨ë¸ê³¼ í•¨ê»˜ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
    )

    if uploaded_file:
        image = Image.open(uploaded_file)
        st.image(image, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", width=300)

# ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    if message["role"] == "user":
        with st.chat_message("user"):
            if message.get("image"):
                st.image(message["image"], width=300)
            st.markdown(message["content"])
    elif message["role"] == "assistant":
        icon = MODEL_ICONS.get(message.get("model_name"), "ğŸ¤–")
        with st.chat_message("assistant", avatar=icon):
            if message.get("model_name"):
                st.markdown(f"**{message['model_name']}**")

            if message.get("content"):
                content = message["content"]
                cleaned_content, has_cjk = detect_and_clean_cjk(content)

                if has_cjk or message.get("has_cjk"):
                    st.warning("âš ï¸ ì¤‘êµ­ì–´/ì¼ë³¸ì–´ í•œì í¬í•¨")
                    st.markdown(cleaned_content)
                else:
                    st.markdown(content)

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° í•¨ê»˜ ì €ì¥
    user_message = {"role": "user", "content": prompt}
    if uploaded_file:
        image = Image.open(uploaded_file)
        user_message["image"] = image

    st.session_state.messages.append(user_message)

    with st.chat_message("user"):
        if uploaded_file:
            st.image(image, width=300)
        st.markdown(prompt)

    # ì¼ë°˜ ì±„íŒ…
    model_name = st.session_state.selected_model
    model_id = AVAILABLE_MODELS[model_name]
    icon = MODEL_ICONS.get(model_name, "ğŸ¤–")

    with st.chat_message("assistant", avatar=icon):
        st.markdown(f"**{model_name}**")

        with st.spinner("ìƒê° ì¤‘..."):
            try:
                system_prompt = f"""You are {model_name} model.

CRITICAL RULES:
- ONLY use Korean (í•œêµ­ì–´) OR English
- NEVER use Chinese (æ±‰å­—), Japanese (æ—¥æœ¬èª), or other languages
- For Korean: Use ONLY Hangul (í•œê¸€), NO Hanja (í•œì)
- Match the user's language (Korean question â†’ Korean answer)"""

                # ì´ë¯¸ì§€ê°€ ìˆê³  Vision ëª¨ë¸ì¸ ê²½ìš°
                is_vision_model = "Vision" in model_name

                if uploaded_file and is_vision_model:
                    # Vision ëª¨ë¸ìš© ë©”ì‹œì§€ êµ¬ì„±
                    image_base64 = encode_image(image)
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{image_base64}"
                                    }
                                }
                            ]
                        }
                    ]
                elif uploaded_file and not is_vision_model:
                    # Vision ëª¨ë¸ì´ ì•„ë‹Œë° ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œëœ ê²½ìš°
                    st.warning("âš ï¸ í˜„ì¬ ëª¨ë¸ì€ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Vision ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt + " (ì°¸ê³ : ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œë˜ì—ˆì§€ë§Œ í˜„ì¬ ëª¨ë¸ì€ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤)"}
                    ]
                else:
                    # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ]

                chat_completion = client.chat.completions.create(
                    messages=messages,
                    model=model_id,
                    temperature=st.session_state.temperature,
                    max_tokens=st.session_state.max_tokens,
                )

                response = chat_completion.choices[0].message.content
                cleaned_response, has_cjk = detect_and_clean_cjk(response)

                if has_cjk:
                    st.error("âš ï¸ í•œì ê°ì§€ë¨")
                    st.markdown(cleaned_response)
                else:
                    st.markdown(response)

                st.session_state.messages.append({
                    "role": "assistant",
                    "model_name": model_name,
                    "content": response,
                    "has_cjk": has_cjk
                })

            except Exception as e:
                error_msg = str(e)
                needs_rerun = False

                if "decommissioned" in error_msg:
                    st.error(f"âš ï¸ {model_name}ëŠ” ì§€ì› ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.session_state.disabled_models.add(model_name)
                    needs_rerun = True
                elif "rate_limit" in error_msg.lower():
                    st.error(f"âš ï¸ í† í° ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                    st.session_state.disabled_models.add(model_name)
                    needs_rerun = True
                elif "model_terms_required" in error_msg or "terms acceptance" in error_msg.lower():
                    st.error(f"âš ï¸ {model_name}ëŠ” ì•½ê´€ ë™ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                    st.info("â„¹ï¸ Groq Consoleì—ì„œ ì•½ê´€ì— ë™ì˜í•˜ë©´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    st.session_state.disabled_models.add(model_name)
                    needs_rerun = True
                elif "does not support chat completions" in error_msg:
                    st.error(f"âš ï¸ {model_name}ëŠ” ì±„íŒ…ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤ (TTS/Audio ì „ìš©).")
                    st.session_state.disabled_models.add(model_name)
                    needs_rerun = True
                else:
                    st.error(f"ì˜¤ë¥˜: {error_msg}")

                # ëª¨ë¸ì´ ë¹„í™œì„±í™”ë˜ì—ˆìœ¼ë©´ ìë™ìœ¼ë¡œ ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì „í™˜
                if needs_rerun:
                    available_models = [m for m in AVAILABLE_MODELS.keys() if m not in st.session_state.disabled_models]
                    if available_models:
                        st.session_state.selected_model = available_models[0]
                        st.info(f"â„¹ï¸ ìë™ìœ¼ë¡œ '{available_models[0]}' ëª¨ë¸ë¡œ ì „í™˜ë©ë‹ˆë‹¤.")
                        # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œê±° (ì˜¤ë¥˜ ë©”ì‹œì§€ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ)
                        if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
                            st.session_state.messages.pop()
                        time.sleep(2)
                        st.rerun()
